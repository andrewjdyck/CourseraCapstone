---
title: "Tidy Topic Modeling for Adjustments"
author: "A.Dyck"
---
  
```{r echo = FALSE}
library(knitr)
opts_chunk$set(message = FALSE, warning = FALSE)
options(width = 100, dplyr.width = 150)
library(ggplot2)
theme_set(theme_bw())
```


#### Setup

```{r packages}
library(dplyr)
library(tidytext)
```

something here.

```{r word_counts}
library(tidytext)
library(stringr)
library(tidyr)
library(tibble)

d <- read.csv('./WriteOffs.csv', stringsAsFactors = FALSE, encoding = 'latin1') %>%
  as_tibble()
data("stop_words")
docs <- d[which(grepl('06', d$REGION)==FALSE), c('ID', 'DESCRIPTION')] %>%
  setNames(c('id', 'text'))

fcc_terms <- data_frame(
  word = c('fcc', 'fac.ca', 'write', 'loan')
)

tidy_docs <- docs %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words) %>%
  anti_join(fcc_terms)

# count words
word_counts <- tidy_docs %>%
  count(id, word, sort = TRUE) 

word_counts
```

```{r}
tidy_docs %>%
  count(word, sort=TRUE)
```

### Latent Dirichlet Allocation with the topicmodels package

Right now this data frame is in a tidy form, with one-term-per-document-per-row. However, the topicmodels package requires a `DocumentTermMatrix` (from the tm package). As described in [this vignette](tidying_casting.html), we can cast a one-token-per-row table into a `DocumentTermMatrix` with tidytext's `cast_dtm`:
  
```{r chapters_dtm}
adjust_dtm <- word_counts %>%
  cast_dtm(id, word, n)

adjust_dtm
```

Now we are ready to use the [topicmodels](https://cran.r-project.org/package=topicmodels) package to create a four topic LDA model.

```{r chapters_lda}
library(topicmodels)
adjust_lda <- LDA(adjust_dtm, k = 4, control = list(seed = 1234))
adjust_lda
```

(In this case I'm trying two topics based on the network diagram; we may need to try a few different values of `k`).

Now tidytext gives us the option of *returning* to a tidy analysis, using the `tidy` and `augment` verbs borrowed from the [broom package](https://github.com/dgrtwo/broom). In particular, we start with the `tidy` verb.

```{r chapters_lda_td}
adjust_lda_td <- tidy(adjust_lda)
adjust_lda_td
```

Notice that this has turned the model into a one-topic-per-term-per-row format. For each combination the model has $\beta$, the probability of that term being generated from that topic.

We could use dplyr's `top_n` to find the top 5 terms within each topic:

```{r top_terms}
top_terms <- adjust_lda_td %>%
  group_by(topic) %>%
  top_n(5, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms
```

This model lends itself to a visualization:

```{r top_terms_plot, fig.height=7, fig.width=7}
library(ggplot2)
theme_set(theme_bw())

top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta)) +
  geom_bar(stat = "identity") +
  facet_wrap(~ topic, scales = "free") +
  theme(axis.text.x = element_text(size = 15, angle = 90, hjust = 1))
```

Does 2 documents look like it fits?

#### Per-document classification

Each adjustment description was a "document" in this analysis. Thus, we may want to know which topics are associated with each document. 

```{r chapters_lda_gamma_raw}
adjust_lda_gamma <- tidy(adjust_lda, matrix = "gamma") %>%
  setNames(c('id', 'topic', 'gamma')) %>%
  mutate(id=as.numeric(id))
adjust_lda_gamma
```

Setting `matrix = "gamma"` returns a tidied version with one-document-per-topic-per-row. Now that we have these document classifiations, we can see how well our unsupervised learning did at distinguishing the adjustment descriptions. 

<!-- First we re-separate the document name into title and chapter: -->
<!-- ```{r chapters_lda_gamma} -->
<!-- chapters_lda_gamma <- chapters_lda_gamma %>% -->
<!--   separate(document, c("title"), sep = "_", convert = TRUE) -->
<!--   chapters_lda_gamma -->
<!-- ``` -->

Then we examine what fraction of chapters we got right for each:

```{r chapters_lda_gamma_plot, fig.width=7, fig.height=6}
ggplot(adjust_lda_gamma, aes(gamma, fill = factor(topic))) +
  geom_histogram() +
  facet_wrap(~ topic, nrow = 2)
```
